# Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA

<!-- TODO: Add paper image/figure here -->
![Method Overview](./readme_assets/hero_figure.png)

<!-- TODO: Add authors with links -->
**Authors**: [Yahya Benmahane](https://www.linkedin.com/in/yahya-benmahane/), [Mohammed El Hassouni](https://scholar.google.com/citations?user=aIwj9L0AAAAJ&hl=fr)

**Paper**: [Link to Paper](https://arxiv.org/abs/xxxx.xxxxx) | **Checkpoints**: [HuggingFace Hub](https://huggingface.co/your-username/visual-prompt-checkpoints)

**Abstract**: In this paper, we propose a novel parameter-efficient adaptation method for No-Reference Image Quality Assessment (NR-IQA) using visual prompts optimized in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models (MLLMs), our approach trains only ~600K parameters at most (<0.01% of the base model), while keeping the underlying model fully frozen. During inference, these visual prompts are combined with images via addition and processed by mPLUG-Owl2 with the textual query "Rate the technical quality of the image." Evaluations across distortion types (synthetic, realistic, AI-generated) on KADID-10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on KADID-10k. To our knowledge, this is the first work to leverage pixel-space visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level vision tasks.

---

This repository implements our proposed method.

## üî• Key Features

- **Parameter-Efficient**: Only ~600K trainable parameters vs 7B+ for full fine-tuning
- **Competitive Performance**: Achieves 0.93 SROCC on KADID-10k dataset
- **Multiple Visual Prompt Types**: Padding, Fixed Patches (Center/Top-Left), Full Overlay
- **Multiple MLLM Support**: mPLUG-Owl2-7B and LLaVA-1.5-7B
- **Comprehensive Evaluation**: Supports KADID-10k, KonIQ-10k, and AGIQA-3k datasets
- **Pre-trained Checkpoints**: Available on HuggingFace Hub for immediate use

## üìã Table of Contents

- [Installation](#installation)
- [Dataset Setup](#dataset-setup)
- [Pre-trained Checkpoints](#pre-trained-checkpoints)
- [Quick Start](#quick-start)
- [Training](#training)
- [Inference](#inference)
- [Configuration](#configuration)
- [Visual Prompt Types](#visual-prompt-types)
- [Results](#results)
- [Contact](#contact)
- [Citation](#citation)

## üöÄ Installation

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (tested on NVIDIA RTX A6000)
- PyTorch
- HuggingFace Transformers

### Setup Environment

#### For mPLUG-Owl2:
```bash
# Clone and setup mPLUG-Owl2
git clone https://github.com/X-PLUG/mPLUG-Owl.git
cd mPLUG-Owl/mPLUG-Owl2
conda create -n mplug_owl2 python=3.10 -y
conda activate mplug_owl2
pip install --upgrade pip
pip install -e .
pip install 'numpy<2'
pip install protobuf
```

#### Additional Dependencies:
```bash
pip install PyYAML scikit-learn tqdm
```

## üìä Dataset Setup

### Download Datasets

Download the required IQA datasets:

```bash
# KonIQ-10k
wget https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets/resolve/main/koniq10k.tgz
tar -xzf koniq10k.tgz

# KADID-10k  
wget https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets/resolve/main/kadid10k.tgz
tar -xzf kadid10k.tgz

# AGIQA-3K
wget https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets/resolve/main/AGIQA-3K.zip
unzip AGIQA-3K.zip
```

### Dataset Organization

After extraction, organize your datasets in the `data/` folder as follows:

```
data/
‚îú‚îÄ‚îÄ kadid10k/
‚îÇ   ‚îú‚îÄ‚îÄ images/           # All KADID-10k images
‚îÇ   ‚îî‚îÄ‚îÄ split_kadid10k.csv
‚îú‚îÄ‚îÄ koniq10k/
‚îÇ   ‚îú‚îÄ‚îÄ 512x384/          # KonIQ-10k images (comes with own split)
‚îÇ   ‚îî‚îÄ‚îÄ koniq10k_*.csv    # Original split files
‚îî‚îÄ‚îÄ AGIQA-3K/
    ‚îú‚îÄ‚îÄ images/           # All AGIQA-3k images
    ‚îî‚îÄ‚îÄ split_agiqa3k.csv
```

**Important Notes:**
- **KADID-10k**: Move `split_kadid10k.csv` into the `kadid10k/` folder
- **KonIQ-10k**: Uses its own original split files, no need to move
- **AGIQA-3k**: Move `split_agiqa3k.csv` into the `AGIQA-3K/` folder; images are in the `images/` subfolder

## ü§ó Pre-trained Checkpoints

We provide pre-trained visual prompt checkpoints on **HuggingFace Hub** for immediate use:

üîó **[Download Checkpoints](https://huggingface.co/your-username/visual-prompt-checkpoints)**

### Available Checkpoints

The checkpoints are provided as `visual_prompt_ckpt_trained_on_mplug2.zip` containing training experiment folders with checkpoint directories (`checkpoint-xxxx`). Each experiment folder contains multiple epochs, and the best performing checkpoint can be identified from the `best_model_checkpoint` info in the final checkpoint folder.

| Dataset | Visual Prompt | Size | Mode | SROCC | Experiment Folder |
|---------|---------------|------|------|-------|-------------------|
| KADID-10k | Padding | 30px | Add | 0.932 | `SGD_mplug2_exp_04_kadid_padding_30px_add/` |
| KonIQ-10k | Padding | 30px | Add | 0.852 | `SGD_mplug2_exp_05_koniq_padding_30px_add/` |
| AGIQA-3k | Padding | 30px | Add | 0.810 | `SGD_mplug2_exp_06_agiqa_padding_30px_add/` |

### Using Pre-trained Checkpoints

1. **Download and extract the checkpoint archive**:
```bash
# Download from HuggingFace Hub
wget https://huggingface.co/your-username/visual-prompt-checkpoints/resolve/main/visual_prompt_ckpt_trained_on_mplug2.zip
unzip visual_prompt_ckpt_trained_on_mplug2.zip
```

2. **Navigate to the desired experiment folder**:
```bash
cd SGD_mplug2_exp_04_kadid_padding_30px_add/
# Check the latest checkpoint folder (highest number)
ls -la checkpoint-*/
# Look for best_model_checkpoint info in the final checkpoint
```

3. **Update the configuration and checkpoint** in `src/tester.py`:
```python

config_path = "configs/final_mplug_owl2_configs/SGD_mplug2_exp_04_kadid_padding_30px_add.yaml"

# - use "checkpoint-best" or specific checkpoint number
checkpoint_best = "checkpoint-best"  # or "checkpoint-XXXX" for specific epoch
```

4. **Run inference** as described in the [Inference](#inference) section.

## üèÉ Quick Start

### 1. Training a Visual Prompt

```bash
cd src
python trainer.py 1
```

This runs experiment #1 from the predefined experiment list. The experiment ID maps to specific configurations in `trainer.py`.

### 2. Custom Training

Create a custom configuration file based on the examples in `configs/`:

```bash
# Copy an existing config
cp configs/mplug_owl2_configs/SGD_mplug2_exp_01_kadid_padding_10px_add.yaml configs/my_experiment.yaml
# Edit the configuration
# Run training by modifying the config path in trainer.py
```

### 3. Inference

```bash
cd src
python tester.py
```

Modify the config path and checkpoint name in `tester.py` for your specific experiment.

## üéØ Training

### Training Process

The training uses HuggingFace's `Trainer` with custom components:

1. **Model**: `VLMWithVisualPrompt` - combines frozen MLLM + trainable visual prompt
2. **Loss**: Mean Squared Error (MSE) between predicted and ground truth quality scores  
3. **Metrics**: SRCC and PLCC correlation coefficients
4. **Optimization**: SGD optimizer with configurable learning rates

### Key Training Parameters

- **Batch Size**: 32 (KADID-10k), 4 (KonIQ-10k, AGIQA-3k)
- **Learning Rate**: 60 (KADID-10k), varies by dataset
- **Epochs**: 25 (KADID-10k), 35 (AGIQA-3k)
- **Optimizer**: SGD
- **Data Augmentation**: Random horizontal flipping
- **Normalization**: Applied after visual prompt addition, before MLLM input

### Training Command

```bash
cd src
python trainer.py <experiment_number>
```

Where `<experiment_number>` corresponds to predefined experiments (1-33):
- All experiments use mPLUG-Owl2-7B with different visual prompt configurations
- Covers all combinations of datasets, prompt types, sizes, and modes

## üîç Inference

### Running Inference

1. **Modify** `src/tester.py` to specify:
   - Configuration file path: `config_path = "configs/final_mplug_owl2_configs/SGD_mplug2_exp_01_kadid_padding_10px_add.yaml"`
   - Checkpoint name: `checkpoint_best = "checkpoint-best"` (or specific checkpoint like "checkpoint-1000")
   
2. **Run inference**:
```bash
cd src
python tester.py
```

### Output

The inference script outputs:
- SRCC (Spearman Rank Correlation Coefficient)
- PLCC (Pearson Linear Correlation Coefficient)

## ‚öôÔ∏è Configuration

### Configuration File Structure

```yaml
experiment_name: "my_experiment"
model:
  vlm_name: "mplug2"  # mPLUG-Owl2-7B
  visual_prompt:
    type: "padding"    # padding, patch_center, patch_topleft, overlay
    args:
      size: 30         # prompt size in pixels
      delta: 1         # scaling factor
      mode: "add"      # add or mul
      is_center: true  # for patch prompts
quality_score_type: "good+fine_vs_poor+bad"
training:
  output_dir: "outputs/my_experiment"
  num_train_epochs: 25
  learning_rate: 60
  per_device_train_batch_size: 32
  # ... other training parameters
dataset:
  train:
    path: "./data"
    name: "kadid"     # kadid, koniq, agiqa
    split: "train"
  # ... eval and test splits
```

### Quality Score Types

Available quality score extraction methods:
- `"good+fine_vs_poor+bad"`
- `"excellent+good_vs_poor+bad"`  
- `"good_vs_bad"`
- `"good_vs_poor"`
- `"fine_vs_poor"`
- `"excellent_vs_bad"`

## üé® Visual Prompt Types

### 1. Padding
Adds learnable pixels around image borders.
- **Sizes**: 10px, 30px
- **Parameters**: `3 √ó 2S(W + H - S)` where S=size, W=width, H=height

### 2. Fixed Patch (Center)
Places a learnable square patch at image center.
- **Sizes**: 10px, 30px  
- **Parameters**: `3 √ó S¬≤`

### 3. Fixed Patch (Top-Left)
Places a learnable square patch at top-left corner.
- **Sizes**: 10px, 30px
- **Parameters**: `3 √ó S¬≤`

### 4. Full Overlay
Applies learnable prompt across entire image.
- **Parameters**: `3 √ó H √ó W`

### Combination Modes
- **Addition**: `prompted_image = image + prompt`
- **Multiplication**: `prompted_image = image √ó prompt`

### Implementation Details
- Visual prompts are restricted to [-1, 1] range using tanh function
- Final pixel values are clamped to [0, 1] range
- Normalization is applied after visual prompt addition

## üìà Results

### Best Performance (30px Padding + Addition)

| Dataset | SROCC | PLCC | Parameters |
|---------|-------|------|------------|
| KADID-10k | 0.932 | 0.929 | ~600K |
| KonIQ-10k | 0.852 | 0.874 | ~600K |
| AGIQA-3k | 0.810 | 0.860 | ~600K |

### Performance Across Visual Prompt Types

| Prompt Type | Size | KADID-10k SROCC | KonIQ-10k SROCC | AGIQA-3k SROCC |
|-------------|------|-----------------|-----------------|----------------|
| Padding | 10px | 0.880 | 0.805 | 0.802 |
| Padding | 30px | **0.932** | **0.852** | **0.810** |
| Fixed Patch (Center) | 10px | 0.390 | 0.487 | 0.435 |
| Fixed Patch (Center) | 30px | 0.806 | 0.647 | 0.725 |
| Fixed Patch (Top-Left) | 10px | 0.465 | 0.551 | 0.564 |
| Fixed Patch (Top-Left) | 30px | 0.520 | 0.635 | 0.755 |
| Full Overlay | ‚Äî | 0.887 | 0.693 | 0.624 |

### Comparison with State-of-the-Art Methods

| Method | KADID-10k SROCC | KonIQ-10k SROCC | AGIQA-3k SROCC | Parameters |
|--------|-----------------|-----------------|----------------|------------|
| **Our Method** | **0.932** | **0.852** | **0.810** | ~600K |
| Q-Align | 0.919 | 0.940 | 0.727 | 7B |
| Q-Instruct | 0.706 | 0.911 | 0.772 | 7B |
| LIQE | 0.930 | 0.919 | - | - |
| MP-IQE | 0.941 | 0.898 | - | - |
| MCPF-IQA | - | 0.918 | 0.872 | - |
| Q-Adapt | 0.769 | 0.878 | 0.757 | - |

### Comparison with Specialized NR-IQA Models

| Method | KADID-10k SROCC | KonIQ-10k SROCC |
|--------|-----------------|-----------------|
| **Our Method** | **0.932** | 0.852 |
| HyperIQA | 0.872 | 0.906 |
| TreS | 0.858 | 0.928 |
| UNIQUE | 0.878 | 0.896 |
| MUSIQ | - | 0.916 |
| DBCNN | 0.878 | 0.864 |

## üóÇÔ∏è Repository Structure

```
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py              # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ tester.py               # Inference script  
‚îÇ   ‚îú‚îÄ‚îÄ vlms_plus_prompters.py  # Combined VLM+Prompt model
‚îÇ   ‚îú‚îÄ‚îÄ vlms.py                 # Individual VLM implementations
‚îÇ   ‚îú‚îÄ‚îÄ prompters.py            # Visual prompt modules
‚îÇ   ‚îú‚îÄ‚îÄ datasets.py             # Dataset loading
‚îÇ   ‚îú‚îÄ‚îÄ collators.py            # Data collation
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                # Utility functions
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ mplug_owl2_configs/     # mPLUG-Owl2 configurations (33 experiments)
‚îú‚îÄ‚îÄ data/                       # Dataset files and splits
‚îú‚îÄ‚îÄ outputs/                    # Training outputs and checkpoints
‚îî‚îÄ‚îÄ README.md
```

## ‚ö†Ô∏è Important Notes

1. **GPU Memory**: Training requires significant GPU memory due to MLLM size
2. **Gradient Flow**: Standard image processors break gradients - our implementation preserves them
3. **Model Focus**: This codebase focuses on mPLUG-Owl2-7B for best performance
4. **Hyperparameters**: Learning rates and batch sizes are dataset-specific
5. **Textual Prompt**: Uses "Rate the technical quality of the image." for consistency
6. **Token Selection**: Positive tokens: "good", "fine"; Negative tokens: "poor", "bad"

## üî¨ Reproducing Results

To reproduce the paper results:

1. **Setup environment** following installation instructions
2. **Download and organize datasets** in `data/` folder following the structure above
3. **Run predefined experiments**:
   ```bash
   # Best results: 30px padding with addition
   python trainer.py 5   # KonIQ-10k + 30px padding
   python trainer.py 4   # KADID-10k + 30px padding  
   python trainer.py 6   # AGIQA-3k + 30px padding
   ```
4. **Evaluate** using `tester.py` with corresponding checkpoints

## üìù Citation

If you use this code in your research, please cite:

```bibtex
@article{}
```

## ü§ù Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes  
4. Submit a pull request

## üìÑ License

...

## üôè Acknowledgments

- [mPLUG-Owl2](https://github.com/X-PLUG/mPLUG-Owl) for the base multimodal LLM
- [LLaVA](https://github.com/haotian-liu/LLaVA) for the alternative MLLM implementation
- HuggingFace Transformers for the training framework
- Visual prompting literature for inspiration
