# PixelPrompt: Parameter-Efficient NR-IQA via Pixel-Space Visual Prompts in Multimodal LLMs

<!-- TODO: Add paper image/figure here -->
![PixelPrompt Overview](./readme_assets/hero_figure.png)

<!-- TODO: Add authors with links -->
**Authors**: [Yahya Benmahane](https://www.linkedin.com/in/yahya-benmahane/), [Mohammed El Hassouni](https://scholar.google.com/citations?user=aIwj9L0AAAAJ&hl=fr)

**Paper**: [Link to Paper](https://arxiv.org/abs/xxxx.xxxxx) | **Checkpoints**: [HuggingFace Hub](https://huggingface.co/your-username/pixelprompt-checkpoints)

**Abstract**: In this paper, we propose PixelPrompt, a novel parameter-efficient adaptation method for No-Reference Image Quality Assessment (NR-IQA) using visual prompts optimized in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models (MLLMs), our approach optimizes a negligible number of learnable parameters while keeping the base MLLM entirely fixed. During inference, these visual prompts are combined with images via addition and processed by the MLLM with the textual query ``Rate the technical quality of the image." Extensive evaluations across distortion types (synthetic, realistic, AI-generated) on KADID-10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against full finetuned methods, achieving 0.91 SROCC on KADID-10k. To our knowledge, this is the first work to leverage pixel-space visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level vision tasks.

---

This repository implements **PixelPrompt**, a novel parameter-efficient adaptation method for No-Reference Image Quality Assessment (NR-IQA) using visual prompts optimized in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models (MLLMs), our approach optimizes a negligible number of learnable parameters while keeping the base MLLM entirely frozen.

## üî• Key Features

- **Parameter-Efficient**: Only 52K trainable parameters vs 7B+ for full fine-tuning
- **Competitive Performance**: Achieves 0.91 SROCC on KADID-10k dataset
- **Multiple Visual Prompt Types**: Padding, Fixed Patches (Center/Top-Left), Full Overlay
- **Multiple MLLM Support**: mPLUG-Owl2-7B and LLaVA-1.5-7B
- **Comprehensive Evaluation**: Supports KADID-10k, KonIQ-10k, and AGIQA-3k datasets
- **Pre-trained Checkpoints**: Available on HuggingFace Hub for immediate use

## üìã Table of Contents

- [Installation](#installation)
- [Dataset Setup](#dataset-setup)
- [Pre-trained Checkpoints](#pre-trained-checkpoints)
- [Quick Start](#quick-start)
- [Training](#training)
- [Inference](#inference)
- [Configuration](#configuration)
- [Visual Prompt Types](#visual-prompt-types)
- [Results](#results)
- [Contact](#contact)
- [Citation](#citation)

## üöÄ Installation

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (tested on NVIDIA RTX A6000)
- PyTorch
- HuggingFace Transformers

### Setup Environment

#### For mPLUG-Owl2:
```bash
# Clone and setup mPLUG-Owl2
git clone https://github.com/X-PLUG/mPLUG-Owl.git
cd mPLUG-Owl/mPLUG-Owl2
conda create -n mplug_owl2 python=3.10 -y
conda activate mplug_owl2
pip install --upgrade pip
pip install -e .
pip install 'numpy<2'
pip install protobuf
```

#### For LLaVA-1.5:
```bash
pip install transformers scipy pandas torch accelerate torchvision
```

#### Additional Dependencies:
```bash
pip install PyYAML scikit-learn tqdm
```

## üìä Dataset Setup

### Download Datasets

Download the required IQA datasets:

```bash
# KonIQ-10k
wget https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets/resolve/main/koniq10k.tgz
tar -xzf koniq10k.tgz

# KADID-10k  
wget https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets/resolve/main/kadid10k.tgz
tar -xzf kadid10k.tgz

# AGIQA-3K
wget https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets/resolve/main/AGIQA-3K.zip
unzip AGIQA-3K.zip
```

### Dataset Organization

After extraction, organize your datasets in the `data/` folder as follows:

```
data/
‚îú‚îÄ‚îÄ kadid10k/
‚îÇ   ‚îú‚îÄ‚îÄ images/           # All KADID-10k images
‚îÇ   ‚îî‚îÄ‚îÄ split_kadid10k.csv
‚îú‚îÄ‚îÄ koniq10k/
‚îÇ   ‚îú‚îÄ‚îÄ 512x384/          # KonIQ-10k images (comes with own split)
‚îÇ   ‚îî‚îÄ‚îÄ koniq10k_*.csv    # Original split files
‚îî‚îÄ‚îÄ AGIQA-3K/
    ‚îú‚îÄ‚îÄ images/           # All AGIQA-3k images
    ‚îî‚îÄ‚îÄ split_agiqa3k.csv
```

**Important Notes:**
- **KADID-10k**: Move `split_kadid10k.csv` into the `kadid10k/` folder
- **KonIQ-10k**: Uses its own original split files, no need to move
- **AGIQA-3k**: Move `split_agiqa3k.csv` into the `AGIQA-3K/` folder; images are in the `images/` subfolder

## ü§ó Pre-trained Checkpoints

We provide pre-trained visual prompt checkpoints on **HuggingFace Hub** for immediate use:

üîó **[Download Checkpoints](https://huggingface.co/your-username/pixelprompt-checkpoints)**

### Available Checkpoints

The checkpoints are provided as `visual_prompt_ckpt_trained_on_mplug2.zip` containing training experiment folders with checkpoint directories (`checkpoint-xxxx`). Each experiment folder contains multiple epochs, and the best performing checkpoint can be identified from the `best_model_checkpoint` info in the final checkpoint folder.

| Dataset | Visual Prompt | Size | Mode | SROCC | Experiment Folder |
|---------|---------------|------|------|-------|-------------------|
| KADID-10k | Padding | 30px | Add | 0.910 | `SGD_mplug2_exp_04_kadid_padding_30px_add/` |
| KonIQ-10k | Padding | 30px | Add | 0.852 | `SGD_mplug2_exp_05_koniq_padding_30px_add/` |
| AGIQA-3k | Padding | 30px | Add | 0.810 | `SGD_mplug2_exp_06_agiqa_padding_30px_add/` |

### Using Pre-trained Checkpoints

1. **Download and extract the checkpoint archive**:
```bash
# Download from HuggingFace Hub
wget https://huggingface.co/your-username/pixelprompt-checkpoints/resolve/main/visual_prompt_ckpt_trained_on_mplug2.zip
unzip visual_prompt_ckpt_trained_on_mplug2.zip
```

2. **Navigate to the desired experiment folder**:
```bash
cd SGD_mplug2_exp_04_kadid_padding_30px_add/
# Check the latest checkpoint folder (highest number)
ls -la checkpoint-*/
# Look for best_model_checkpoint info in the final checkpoint
```

3. **Update the checkpoint path** in `src/tester.py`:
```python
visual_prompt_path = "SGD_mplug2_exp_04_kadid_padding_30px_add/checkpoint-XXXX/visual_prompt.pth"
```

4. **Run inference** as described in the [Inference](#inference) section.

## üèÉ Quick Start

### 1. Training a Visual Prompt

```bash
cd src
python trainer.py 1
```

This runs experiment #1 from the predefined experiment list. The experiment ID maps to specific configurations in `trainer.py`.

### 2. Custom Training

Create a custom configuration file based on the examples in `configs/`:

```bash
# Copy an existing config
cp configs/mplug_owl2_configs/SGD_mplug2_exp_01_kadid_padding_10px_add.yaml configs/my_experiment.yaml
# Edit the configuration
# Run training by modifying the config path in trainer.py
```

### 3. Inference

```bash
cd src
python tester.py
```

Modify the config path and checkpoint path in `tester.py` for your specific experiment.

## üéØ Training

### Training Process

The training uses HuggingFace's `Trainer` with custom components:

1. **Model**: `VLMWithVisualPrompt` - combines frozen MLLM + trainable visual prompt
2. **Loss**: Mean Squared Error (MSE) between predicted and ground truth quality scores  
3. **Metrics**: SROCC and PLCC correlation coefficients
4. **Optimization**: SGD optimizer with configurable learning rates

### Key Training Parameters

- **Batch Size**: 32 (KADID-10k), 4 (KonIQ-10k, AGIQA-3k)
- **Learning Rate**: 60 (KADID-10k), varies by dataset
- **Epochs**: 25 (KADID-10k), 35 (AGIQA-3k)
- **Optimizer**: SGD

### Training Command

```bash
cd src
python trainer.py <experiment_number>
```

Where `<experiment_number>` corresponds to predefined experiments (1-33):
- All experiments use mPLUG-Owl2-7B with different visual prompt configurations
- Covers all combinations of datasets, prompt types, sizes, and modes

## üîç Inference

### Running Inference

1. **Modify** `src/tester.py` to specify:
   - Configuration file path
   - Checkpoint path for trained visual prompt
   
2. **Run inference**:
```bash
cd src
python tester.py
```

### Output

The inference script outputs:
- SROCC (Spearman Rank Correlation Coefficient)
- PLCC (Pearson Linear Correlation Coefficient)
- Individual predictions vs ground truth

## ‚öôÔ∏è Configuration

### Configuration File Structure

```yaml
experiment_name: "my_experiment"
model:
  vlm_name: "mplug2"  # mPLUG-Owl2-7B
  visual_prompt:
    type: "padding"    # padding, patch_center, patch_topleft, overlay
    args:
      size: 30         # prompt size in pixels
      delta: 1         # scaling factor
      mode: "add"      # add or mul
      is_center: true  # for patch prompts
quality_score_type: "good+fine_vs_poor+bad"
training:
  output_dir: "outputs/my_experiment"
  num_train_epochs: 25
  learning_rate: 0.01
  per_device_train_batch_size: 32
  # ... other training parameters
dataset:
  train:
    path: "./data"
    name: "kadid"     # kadid, koniq, agiqa
    split: "train"
  # ... eval and test splits
```

### Quality Score Types

Available quality score extraction methods:
- `"good+fine_vs_poor+bad"`
- `"excellent+good_vs_poor+bad"`  
- `"good_vs_bad"`
- `"good_vs_poor"`
- `"fine_vs_poor"`
- `"excellent_vs_bad"`

## üé® Visual Prompt Types

### 1. Padding
Adds learnable pixels around image borders.
- **Sizes**: 10px, 30px
- **Parameters**: `3 √ó 2S(W + H - S)` where S=size, W=width, H=height

### 2. Fixed Patch (Center)
Places a learnable square patch at image center.
- **Sizes**: 10px, 30px  
- **Parameters**: `3 √ó S¬≤`

### 3. Fixed Patch (Top-Left)
Places a learnable square patch at top-left corner.
- **Sizes**: 10px, 30px
- **Parameters**: `3 √ó S¬≤`

### 4. Full Overlay
Applies learnable prompt across entire image.
- **Parameters**: `3 √ó H √ó W`

### Combination Modes
- **Addition**: `prompted_image = image + prompt`
- **Multiplication**: `prompted_image = image √ó prompt`

## üìà Results

### Best Performance (30px Padding + Addition)

| Dataset | SROCC | PLCC | Parameters |
|---------|-------|------|------------|
| KADID-10k | 0.910 | 0.905 | 52K |
| KonIQ-10k | 0.852 | 0.874 | 52K |
| AGIQA-3k | 0.810 | 0.860 | 52K |

### Comparison with Full Fine-tuning

| Method | Parameters | KADID-10k SROCC |
|--------|------------|------------------|
| PixelPrompt | 52K | 0.910 |
| Q-Align | 7B | 0.919 |
| Q-Instruct | 7B | 0.706 |

## üóÇÔ∏è Repository Structure

```
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py              # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ tester.py               # Inference script  
‚îÇ   ‚îú‚îÄ‚îÄ vlms_plus_prompters.py  # Combined VLM+Prompt model
‚îÇ   ‚îú‚îÄ‚îÄ vlms.py                 # Individual VLM implementations
‚îÇ   ‚îú‚îÄ‚îÄ prompters.py            # Visual prompt modules
‚îÇ   ‚îú‚îÄ‚îÄ datasets.py             # Dataset loading
‚îÇ   ‚îú‚îÄ‚îÄ collators.py            # Data collation
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                # Utility functions
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ mplug_owl2_configs/     # mPLUG-Owl2 configurations (33 experiments)
‚îú‚îÄ‚îÄ data/                       # Dataset files and splits
‚îú‚îÄ‚îÄ outputs/                    # Training outputs and checkpoints
‚îî‚îÄ‚îÄ README.md
```

## ‚ö†Ô∏è Important Notes

1. **GPU Memory**: Training requires significant GPU memory due to MLLM size
2. **Gradient Flow**: Standard image processors break gradients - our implementation preserves them
3. **Model Focus**: This codebase focuses on mPLUG-Owl2-7B for best performance
4. **Hyperparameters**: Learning rates and batch sizes are dataset-specific

## üî¨ Reproducing Results

To reproduce the paper results:

1. **Setup environment** following installation instructions
2. **Download and organize datasets** in `data/` folder following the structure above
3. **Run predefined experiments**:
   ```bash
   # Best results: 30px padding with addition
   python trainer.py 5   # KonIQ-10k + 30px padding
   python trainer.py 4   # KADID-10k + 30px padding  
   python trainer.py 6   # AGIQA-3k + 30px padding
   ```
4. **Evaluate** using `tester.py` with corresponding checkpoints

## üìù Citation

If you use this code in your research, please cite:

```bibtex
@article{pixelprompt2024,
  title={PixelPrompt: Parameter-Efficient NR-IQA via Pixel-Space Visual Prompts in Multimodal LLMs},
  author={Yahya Benmahane and Mohammed El Hassouni},
  journal={[Journal/Conference]},
  year={2024}
}
```

## ü§ù Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes  
4. Submit a pull request

## üìÑ License

...

## üôè Acknowledgments

- [mPLUG-Owl2](https://github.com/X-PLUG/mPLUG-Owl) for the base multimodal LLM
- [LLaVA](https://github.com/haotian-liu/LLaVA) for the alternative MLLM implementation
- HuggingFace Transformers for the training framework
- [I have to reference visual prompting]
